# MNIST Neural Network Architecture Exploration

## Homework Questions and Answers

### Q1. Based on the model summary, calculate and justify the number of parameters.

The original model architecture:
```python
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(784,)))  # [1]
model.add(Dense(64, activation='relu'))  # [2]
model.add(Dense(10, activation='softmax'))
```

**Parameter calculations:**

- **First layer (Dense 784→64)**: 50,240 parameters
  - Weights: 784 × 64 = 50,176
  - Biases: 64
  - Total: 50,176 + 64 = 50,240

- **Second layer (Dense 64→64)**: 4,160 parameters
  - Weights: 64 × 64 = 4,096
  - Biases: 64
  - Total: 4,096 + 64 = 4,160

- **Output layer (Dense 64→10)**: 650 parameters
  - Weights: 64 × 10 = 640
  - Biases: 10
  - Total: 640 + 10 = 650

**Total parameters**: 55,050

### Q2. Add Batch Normalization and 20% Dropout following [1] and [2]. Compare the results from Q1.

Modified architecture with regularization:
```python
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(784,)))  # [1]
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))  # [2]
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))
```

**Results comparison:**
- Original model: 55,050 parameters (all trainable)
- Regularized model: 55,562 parameters
  - 55,306 trainable parameters
  - 256 non-trainable parameters (from BatchNorm)

The BatchNormalization layers add 256 parameters each:
- 128 trainable parameters (gamma and beta for each neuron)
- 128 non-trainable parameters (moving mean and variance)

The regularized model maintains similar performance (96.99% vs 97.01% accuracy) while potentially offering better generalization through regularization techniques.

### Q3. Let's build a deeper network by (i) increasing neurons from 64 to 128, or (ii) adding another dense layer after [1]. Compare the results from Q2.

**Option (i): Wider network (128 neurons)**
```python
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(784,)))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))
```

**Option (ii): Deeper network (additional layer)**
```python
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(784,)))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))  # Added layer
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))
```

**Comparison:**

| Model | Parameters | Accuracy | Δ Parameters |
|-------|------------|----------|--------------|
| Q2 Baseline | 55,562 | 96.78% | - |
| Wider (128 neurons) | 119,306 | 97.50% | +63,744 |
| Deeper (extra layer) | 59,978 | 96.86% | +4,416 |

The wider model achieves the best accuracy but requires significantly more parameters. The deeper model offers a more parameter-efficient improvement with a modest accuracy gain.
